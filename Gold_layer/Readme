ğŸ† Gold Layer â€“ E-Commerce Data Lakehouse
This notebook implements the Gold Layer transformations for the project:
â€œE-Commerce Data Lakehouse with AI-Powered Self-Healing Pipelinesâ€ âœ¨
The Gold Layer enriches curated Silver data into SCD-style dimensions and a Fact table that drives analytics, BI dashboards, and ML features.


ğŸ”„ High-Level Flow
1ï¸âƒ£ Load Silver Delta tables:
customers, products, orders, deliveries


2ï¸âƒ£ Build Dimensions (SCD Type-2):
dim_customers ğŸ‘¤
dim_products ğŸ“¦
dim_orders ğŸ§¾
dim_deliveries ğŸšš

3ï¸âƒ£ Enrich & Upsert with Delta MERGE
Ensure dimensions stay up to date with slowly changing history


4ï¸âƒ£ Build Fact Table
fact_sales ğŸ’° by joining/enriching orders, deliveries & dimensions


ğŸ“‚ Outputs / Tables Created
Dimensions
dim_customers
dim_products
dim_orders
dim_deliveries
Fact Table
fact_sales
All stored as Delta tables in the Gold layer.


ğŸ—ï¸ Building Dimensions (SCD Type-2)
For each dimension:
Partition by the business key (customer_id, product_id, etc.)
Order rows by last_updated (DESC)
Pick latest row â†’ mark as is_current = True âœ…
Older rows â†’ mark as is_current = False âŒ


âœ¨ Add metadata:
start_date, end_date
is_current flag
ingestion_timestamp
Surrogate key: customer_sk, product_sk, etc.

ğŸ“Œ Write to:
<base_gold_path>/dim_<table_name>/data


âš¡ Enrichment & Upsert (Delta MERGE)
Example: Deliveries ğŸšš
Deduplicate incoming rows â†’ enriched_deliveries
Use DeltaTable.merge():
âœ… If record unchanged â†’ keep existing
ğŸ”„ If updated â†’ set old is_current = False & insert new version
â• Insert brand-new rows
This ensures Type-2 history tracking across dimensions.


ğŸ“Š Building fact_sales
Fact table = ğŸ”— link between Orders + Deliveries + Dimensions
Joins orders & deliveries with dimension surrogate keys


Keeps key fields like:
transaction_id, customer_sk, product_sk, order_sk, delivery_sk
total_amount, payment_method, delivery fields
Supports BI tools & ML feature engineering


ğŸ“Œ Write to:
<base_gold_path>/fact_sales/data


ğŸ› ï¸ If schema mismatch occurs:
Old folder is dropped & recreated with:
CREATE TABLE IF NOT EXISTS fact_sales 
USING DELTA 
LOCATION '<path>'
With .option("overwriteSchema", "true")


âœ… Validation
After writing, the notebook runs:
.printSchema() â†’ verify schema
.count() â†’ verify row counts
Null checks â†’ ensure data quality


ğŸš€ Next Steps
ğŸ“ˆ Connect Gold tables to Snowflake
